---
title: "Data Scientist Job Posting Data Analysis"
subtitle: "From Glassdoor"
author: "Miray Cinar and Ecenaz Tunc"
format: html
editor: visual
toc: true
highlight-style: "breeze"
eval: true
echo: true
theme: "lux"
bibliography: references.bib
---

## Disclaimer

This document is a product of the final project for the STAT 570 lecture, focusing on data handling and visualization tools. It is essential to acknowledge that minor errors may be present, and the methods employed may not necessarily reflect the optimal approach related to the data set.

Authors:

Miray Çınar - miray.cinar\@metu.edu.tr

Ecenaz Tunç - ecenaz.tunc\@metu.edu.tr

## Introduction

Glassdoor is an online platform where former or new employees can comment on companies and is also used for job search.

In their website, they define themselves as: "Glassdoor is one of the world's largest job and recruiting sites. We pride ourselves on helping people find a job and company they love; in fact, it's our mission. Our company was built on the foundation of increasing workplace transparency. With that in mind, we have developed numerous tools to help job seekers make more informed career decisions."

## Data Description

Data is obtained from Kaggle, in which, the user claims that the data is obtained from Glassdoor.com by using web scrapping. The link for the data could be acquired from the References section.

The variables in this data set are defined as follows:

**Job Title:** Title of the job posting

**Salary Estimation:** Salary range for that particular job

**Job Description:** This contains the full description of that job

**Rating:** Rating of that post

**Company:** Name of company

**Location:** Location of the company

**Headquarter:** Location of the headquater

**Size:** Total employee in that company

**Type of ownership:** Describes the company type i.e non-profit/public/private farm etc

**Industry, Sector:** Field applicant will work in

**Revenue:** Total revenue of the company

## Import the Data

Let's start by importing the data. We tried to import the data from Kaggle to Rstudio directly, by using several different packages but unfortunately for us, it was not possible. But we uploaded the data into out Github repository, so you can directly obtain it from there or from the Kaggle link that we put in the references. In the Kaggle, you can see that there are actually tow datasets, one is already cleaned, and one it unclean. We used the Uncleaned one here . :)

First, import the required libraries. If you don't already have them, you can use `install.packages()` function.

```{r}

library(tidyverse)
library(readr)
library(ggplot2)

Uncleaned_DS_jobs <- read_csv("Uncleaned_DS_jobs.csv", show_col_types = F)
```

Let's start by investigating our dataset a little bit, by getting a glimpse and see the structure of the data:

```{r}
library(dplyr)
glimpse(Uncleaned_DS_jobs)
```

And also take a quick summary:

```{r}
summary(Uncleaned_DS_jobs)
```

From both `glimpse()` and `summary()` outputs, we can see that, categorical variables are in character form. We will investigate them one by one later on.

But first, let's change the column names that have blank spaces so that it will be much easy to make the analyses later.

```{r}
Uncleaned_DS_jobs <- Uncleaned_DS_jobs %>% 
  rename(Job_Title = `Job Title`, 
         Salary_Estimate = `Salary Estimate`,
         Job_Description = `Job Description`,
         Company_Name = `Company Name`, 
         Type_of_Ownership =  `Type of ownership`)
```

Take a summary again to see the data:\

```{r}
summary(Uncleaned_DS_jobs)
```

From our summary, we can also see some strange values are present in the data. For instance there are some rows marked as "-1" in the Headquarters, Founded.

## Investigating the Columns

Let's investigate the columns one by one:

-   Index

Index column is not necessary for us, so we will remove it from our data set.

```{r}
Uncleaned_DS_jobs$index <- NULL
```

-   Rating

We realized that from the summary, Rating has a minimum value as -1, but the rating should be between 1 to 5.

We need to fix that problem.

To fix this, first we need to look how many data are there with Rating = -1:

```{r}
sum(Uncleaned_DS_jobs$Rating == -1)
```

We have 50 values with Rating = -1. Rating variable should not be -1. So firstly for the rating variable we give change -1 to 0.

```{r}

Uncleaned_DS_jobs$Rating[Uncleaned_DS_jobs$Rating == -1] <- 0
```

Now lets check if it worked,

```{r}
summary(Uncleaned_DS_jobs$Rating)
```

As can be seen from the summary of the rating we fix the -1 problem.

From the histogram of the rating variable below, we can see that majority of the companies receieved 4 on the rating.

```{r}
intervals <- seq(0, 5, by = 1)

rating_histogram <- ggplot(Uncleaned_DS_jobs, aes(x = Rating)) +
  geom_histogram(binwidth = 1, boundary = 0.5, col = "darkblue", fill = "lightblue") +
  labs(
    title = "Distribution of Rating",
    x = "Rating",
    y = "Frequency",
    subtitle = ""
  ) +
  scale_x_continuous(breaks = intervals) +  
  theme_classic()
rating_histogram
```

-   Founded

After looking in a more detailed way, we realize that the foundation year of the companies have a value -1 also we need check for them:

```{r}
sum(Uncleaned_DS_jobs$Founded == -1)
```

```{r}
Uncleaned_DS_jobs$Founded[Uncleaned_DS_jobs$Founded == -1] <- "No information"
```

```{r}
summary(as.factor(Uncleaned_DS_jobs$Founded), maxsum = 6)
```

From the summary we also fix the problem for Founded variable.

-   Industry

```{r}
sum(Uncleaned_DS_jobs$Industry == -1)
```

We see that industry has 71 -1 values so, again we assign those values to no information.

```{r}
Uncleaned_DS_jobs$Industry[Uncleaned_DS_jobs$Industry == -1] <- "No information"
```

Histogram of the Industry Variable:

```{r}
industry_plot<- ggplot(Uncleaned_DS_jobs, aes(x=Industry)) + 
  labs(title = "Distribution of Industry", x = "Industry", subtitle = "") + 
  geom_bar(colour="darkblue", fill="lightblue") + 
  geom_text(stat='count', aes(label=..count..), vjust=-0.5,size=2.68) +
  theme_classic()+
   theme(axis.text.x = element_text(size = 6, angle = 45, hjust = 1))
industry_plot
```

As can be seen from the graph it is hard to read the x-axis names, so to solve this problem, we picked the 10 industries that have the most frequencies in the data and draw a plot regarding these industries.

```{r}
top10_industries <- Uncleaned_DS_jobs %>%
  group_by(Industry) %>%
  summarise(count = n()) %>%
  arrange(desc(count)) %>%
  top_n(10)

# Reorder the levels of Industry based on frequency
plot_data <-  Uncleaned_DS_jobs
plot_data$Industry <- factor(plot_data$Industry, levels = top10_industries$Industry)

# Filter data to include only the top 10 industries
filtered_data <- plot_data %>%
  filter(Industry %in% top10_industries$Industry)

industry_plot_top10 <- ggplot(filtered_data, 
                              aes(x = Industry)) +
  labs(title = "Distribution of Top 10 Industries", 
       x = "Industry", subtitle = "") +
  geom_bar(colour = "darkblue", fill = "lightblue") +
  geom_text(stat = 'count', aes(label = ..count..), vjust = -0.5, size = 2.68) +
  theme_classic() +
  theme(axis.text.x = element_text(size = 10, angle = 45, hjust = 1))


industry_plot_top10
```

As can be seen from the graph that, top 10 industries that the companies are in; Biotech & Pharmaceuticals, IT Services, Computer Hardware & Software and so on.

-   Sector

Realizing that sector variable also has -1 values.

```{r}
sum(Uncleaned_DS_jobs$Sector == -1) 
```

We change -1 values to no information for the sector variable.

```{r}
Uncleaned_DS_jobs$Sector[Uncleaned_DS_jobs$Sector == -1] <- "No information"
```

Histogram of the Sector Variable:

```{r}
top10_sector <- Uncleaned_DS_jobs %>%   
  group_by(Sector) %>%   
  summarise(count = n()) %>%   
  arrange(desc(count)) %>%   
  top_n(10)  

# Reorder the levels of Sector based on frequency 
plot_data <-  Uncleaned_DS_jobs
plot_data$Sector <- factor(plot_data$Sector, 
                           levels = top10_sector$Sector)  

# Filter data to include only the top 10 sector 
filtered_data <- plot_data %>%   
  filter(Sector %in% top10_sector$Sector)  

Sector_plot_top10 <- ggplot(filtered_data, aes(x = Sector)) +   
  labs(title = "Distribution of Top 10 Sector", x = "Sector", subtitle = "") +    
  geom_bar(colour = "darkblue", fill = "lightblue") +  
  geom_text(stat = 'count', aes(label = ..count..), vjust = -0.5, size = 2.68) +   
  theme_classic() +   
  theme(axis.text.x = element_text(size = 10, angle = 45, hjust = 1))   

Sector_plot_top10
```

Again for the sectors, similar to the industries, we had a lot of categories. So we decided to show the top 10: it can be seen that Information Technology has the highest job openings, followed by Business Services and again Biotech & Pharmaceuticals and so on.

-   Revenue

In revenue column there are some -1 values

```{r}
sum(Uncleaned_DS_jobs$Revenue == -1)
```

And this column has a value called Unknown / Non-Applicable

```{r}
head(Uncleaned_DS_jobs$Revenue)
```

We convert -1 to that value.

```{r}
Uncleaned_DS_jobs$Revenue[Uncleaned_DS_jobs$Revenue == -1] <- "Unknown / Non-Applicable"  
```

Histogram of the Revenue variable:

```{r}
revenue_plot<- ggplot(Uncleaned_DS_jobs, aes(x=Revenue)) + 
  labs(title = "Distribution of Revenue", x = "Revenue", subtitle = "") + 
  geom_bar(colour="darkblue", fill="lightblue") + 
  geom_text(stat='count', aes(label=..count..), vjust=-0.5,size=2.2) +
  theme_classic()+
   theme(axis.text.x = element_text(size = 9.2, angle = 45, hjust = 1))
revenue_plot

```

For the Revenue variable, from the graph it can be seen that unfortunately we had a lot of missing data. But among the non-missing ones, the highest frequency that we detected in our data set was for the companies with \$100 to \$500 million revenue.

-   Company Name

When we check the Company Name variable, we see that it also has the Rating next to it:

```{r}
head(Uncleaned_DS_jobs[, c("Company_Name", "Rating")])
```

We can separate them and get rid of the Rating variable inside of Company Name to clean this variable. We can do this by `str_replace()` function.

```{r}
Uncleaned_DS_jobs$Company_Name <- str_replace(Uncleaned_DS_jobs$Company_Name, "\n[0-9.]+$", "")
```

Now we have cleaned the Company Name variable:

```{r}
head(Uncleaned_DS_jobs[, c("Company_Name", "Rating")])
```

And we can see that how many of the Company Names:

```{r}
Uncleaned_DS_jobs |> 
  count(Company_Name, sort = TRUE)
```

We can see that the company with the most positions opened is "Hatch Data Inc" and "Maxar Technologies" with 12 positions opened.

Histogram of the Company name:

Same problem occurred here like industries so, to see the plot again the 10 company names with the highest frequencies.

Histogram of the company name:

```{r}
top10_companies <- Uncleaned_DS_jobs %>%
  group_by(Company_Name) %>%
  summarise(count = n()) %>%
  arrange(desc(count)) %>%
  top_n(10)

# Reorder the levels of Company names based on frequency
plot_data <-  Uncleaned_DS_jobs
plot_data$Company_Name <- factor(plot_data$Company_Name, levels = top10_companies$Company_Name)

# Filter data to include only the top 10 industries
filtered_data <- plot_data %>%
  filter(Company_Name %in% top10_companies$Company_Name)

company_plot_top10 <- ggplot(filtered_data, aes(x = Company_Name)) +
  labs(title = "Distribution of Top 10 Companies", x = "Company Name", subtitle = "") +
  geom_bar(colour = "darkblue", fill = "lightblue") +
  geom_text(stat = 'count', aes(label = ..count..), vjust = -0.5, size = 2.68) +
  theme_classic() +
  theme(axis.text.x = element_text(size = 10, angle = 45, hjust = 1))

company_plot_top10
```

-   Size

As can be seen from the summary that we have -1 for the Size. But we have unknown category for this variable.

```{r}
summary(as.factor(Uncleaned_DS_jobs$Size))
```

So we can assign "-1" to "Unknown" category for this variable:

```{r}
Uncleaned_DS_jobs$Size[Uncleaned_DS_jobs$Size == -1] <- "Unknown"
```

```{r}
summary(as.factor(Uncleaned_DS_jobs$Size))
```

Histogram of the Size variable:

```{r}
library(dplyr)

# Create a frequency table for Size
size_counts <- count(Uncleaned_DS_jobs, Size)

# Sort the data by count in ascending order
size_counts <- arrange(size_counts, desc(n))

# Create the plot
size_plot <- ggplot(size_counts, aes(x = reorder(Size, n), y = n)) + 
  labs(title = "Distribution of Size of the Companies", x = "Size", y = "Frequency") + 
  geom_col(colour = "darkblue", fill = "lightblue") + 
  geom_text(aes(label = n), vjust = -0.5, size = 3) +
  theme_classic() +
  theme(axis.text.x = element_text(size = 9.2, angle = 45, hjust = 1))

size_plot

```

Interestingly, from the plot we can see that the size of the companies in terms of employees, we see the highest frequency in 51 to 200 employees, however, other categories also have numbers close to each other.

-   Competitors

There are -1 values in Competitors. We don't know their competitors' name so we can attribute them to no information

```{r}

Uncleaned_DS_jobs$Competitors[Uncleaned_DS_jobs$Competitors == "-1"] <- "No information"

```

```{r}
summary(as.factor(Uncleaned_DS_jobs$Competitors), maxsum = 6)
```

For the competitor companies of the job posting companies we have a lot of different values, but the most repetitive ones could be seen from the output above.

-   Location

Let's see the location variable first.

```{r}
summary(as.factor(Uncleaned_DS_jobs$Location), maxsum = 6)
```

In the Location variable, we can see that they are written with the state which they are in. So we want to separate them. But,before that, in our data there are some problematic rows:

Some rows are too short. Let's see that columns:

```{r}
Uncleaned_DS_jobs %>% 
  filter(
    str_count(Location, ",\\s+") != 1
    ) %>% 
  select(Location) %>% distinct_all()
```

From this output, we can see that we have "Remote", "United States", locations that have the same names as their states; "Utah", "New Jersey", "Texas" and "California", and "Patuxent, Anne Arundel, MD" which is a region for the Anne Arundel county. So, we will add information for this columns firstly, then we will separate the Location and States. For this, we will use `str_replace()` function.

```{r}
# Define replacements using case_when
Uncleaned_DS_jobs <- Uncleaned_DS_jobs %>%
  mutate(
    Location = case_when(
      Location == "Remote" ~ str_replace(Location, "Remote", "Remote, R"),
      Location == "United States" ~ str_replace(Location, "United States", "United States, US"),
      Location == "Utah" ~ str_replace(Location, "Utah", "Utah, UT"),
      Location == "New Jersey" ~ str_replace(Location, "New Jersey", "New Jersey, NJ"),
      Location == "Texas" ~ str_replace(Location, "Texas", "Texas, TX"),
      Location == "California" ~ str_replace(Location, "California", "California, CA"),
      Location == "Patuxent, Anne Arundel, MD" ~ str_replace(Location, "Patuxent, Anne Arundel, MD", "Anne Arundel, MD"),
      TRUE ~ Location
    )
  )
```

Now we can separate them:

```{r}
Uncleaned_DS_jobs <- Uncleaned_DS_jobs %>%
  separate(
    Location,
    into = c("Location", "Location_State"),
    sep = ",\\s+")
  
```

Let's visualize Location_State variable:

```{r}

locationstates_counts <- count(Uncleaned_DS_jobs, Location_State)

# Sort the data by count in ascending order
locationstates_counts <- arrange(locationstates_counts, desc(n))

# Create the plot
locationstate_plot <- ggplot(locationstates_counts, aes(x = reorder(Location_State, n), y = n)) + 
  labs(title = "Distribution of States of the Locations of the Companies", x = "States", y = "Frequency") + 
  geom_col(colour = "darkblue", fill = "lightblue") + 
  geom_text(aes(label = n), vjust = -0.5, size = 3) +
  theme_classic() +
  theme(axis.text.x = element_text(size = 9.2, angle = 45, hjust = 1))
locationstate_plot
```

We can see that majority of the states for the job postings are from California.

-   Headquarters

For the headquarters we have -1 values also,

```{r}
sum(Uncleaned_DS_jobs$Headquarters == -1)

```

```{r}
Uncleaned_DS_jobs$Headquarters[Uncleaned_DS_jobs$Headquarters == "-1"] <- "No information"
```

```{r}
summary(as.factor(Uncleaned_DS_jobs$Headquarters), maxsum = 6)
```

Similar to Location, we want to separate the states and the city. We will apply the similar approach to fix this column.

```{r}
Uncleaned_DS_jobs %>% 
  filter(
    str_count(Headquarters, ",\\s+") != 1
    ) %>% 
  select(Location) %>% distinct_all()
```

We will assign the states for this rows too. and also to not get a warning regarding the NA values later, "No Information" is also going to be fixed:

```{r}
Uncleaned_DS_jobs <- Uncleaned_DS_jobs %>%
  mutate(
    Headquarters = case_when(
      Headquarters == "Hauppauge" ~ str_replace(Headquarters, "Hauppauge", "Hauppauge, NY"),
      Headquarters == "Reston" ~ str_replace(Headquarters, "Reston", "Reston, VA"),
      Headquarters == "New York" ~ str_replace(Headquarters, "New York", "New York, NY"),
      Headquarters == "Palo Alto" ~ str_replace(Headquarters, "Palo Alto", "Palo Alto, CA"),
      Headquarters == "San Francisco" ~ str_replace(Headquarters, "San Francisco", "San Francisco, CA"),
      Headquarters == "Brooklyn" ~ str_replace(Headquarters, "Brooklyn", "Brooklyn, NY"),
      Headquarters == "Sterling" ~ str_replace(Headquarters, "Sterling", "Sterling, IL"),
      Headquarters == "Chantilly" ~ str_replace(Headquarters, "Chantilly", "Chantilly, VA"),
      Headquarters == "Cambridge" ~ str_replace(Headquarters, "Cambridge", "Cambridge, MA"),
      Headquarters == "Fort Belvoir" ~ str_replace(Headquarters, "Fort Belvoir", "Fort Belvoir, VA"),
      Headquarters == "Naperville" ~ str_replace(Headquarters, "Naperville", "Naperville, IL"),
      Headquarters == "Redmond" ~ str_replace(Headquarters, "Redmond", "Redmond, WA"),
      Headquarters == "Irwindale" ~ str_replace(Headquarters, "Irwindale", "Irwindale, CA"),
      Headquarters == "No information" ~ str_replace(Headquarters, "No information", "No information, No information"),
      TRUE ~ Headquarters
    )
  )
```

And finally, we will separate the Headquarters and their states:

```{r}
Uncleaned_DS_jobs <- Uncleaned_DS_jobs %>%
  separate(
    Headquarters,
    into = c("Headquarters", "Headquarters_State"),
    sep = ",\\s+")
  
```

Let's visualize Headquarters States:

```{r}
hqstates_counts <- count(Uncleaned_DS_jobs, Headquarters_State)

# Sort the data by count in ascending order
hqstates_counts <- arrange(hqstates_counts, desc(n))

# Create the plot
hqstate_plot <- ggplot(hqstates_counts, aes(x = reorder(Headquarters_State, n), y = n)) + 
  labs(title = "Distribution of States of the Locations of the Headquarters", x = "States", y = "Frequency") + 
  geom_col(colour = "darkblue", fill = "lightblue") + 
  geom_text(aes(label = n), vjust = -0.5, size = 3) +
  theme_classic() +
  theme(axis.text.x = element_text(size = 9.2, angle = 45, hjust = 1))
hqstate_plot
```

We can see that majority of the states for the job postings companies' headquartes are located in California. But we can also see that some of Campanies' Headqurters are located in other countries like UK, Switzerland, France, Canada etc.

-   Type Of Ownership

There are -1 values in the Type of Ownership also.

We check how many -1 values are in the variable.

```{r}
sum(Uncleaned_DS_jobs$Type_of_Ownership == -1)
```

```{r}
Uncleaned_DS_jobs$Type_of_Ownership[Uncleaned_DS_jobs$Type_of_Ownership == "-1"] <- "Unknown"
```

Let's see the summary:

```{r}
summary(as.factor(Uncleaned_DS_jobs$Type_of_Ownership))
```

Let's visualize type of ownerships of the companies:

```{r}
Type_of_Ownership_counts <- count(Uncleaned_DS_jobs, Type_of_Ownership)

# Sort the data by count in ascending order
Type_of_Ownership_counts <- arrange(Type_of_Ownership_counts, desc(n))

# Create the plot
tow_plot <- ggplot(Type_of_Ownership_counts, aes(x = reorder(Type_of_Ownership, n), y = n)) + 
  labs(title = "Distribution of Type of Ownerships of the Companies", x = "Type of Ownership", y = "Frequency") + 
  geom_col(colour = "darkblue", fill = "lightblue") + 
  geom_text(aes(label = n), vjust = -0.5, size = 3) +
  theme_classic() +
  theme(axis.text.x = element_text(size = 9.2, angle = 45, hjust = 1))
tow_plot
```

We can see that majority, more than half of the companies are made up of private companies, followed by public and nonprofit organizations.

-   Salary Estimation

For Salary Estimate column, let's see the unique values we have:

```{r}
levels(as.factor(Uncleaned_DS_jobs$Salary_Estimate))
```

```{r}
sum(is.na(as.factor(Uncleaned_DS_jobs$Salary_Estimate)))
```

From this output, we can see that we have common shape for the salary estimates with 0 NA values. We can separate this column into two separate columns for obtaining lower and upper limits for the salary estimates.

```{r}
# Remove spaces in the column 
Uncleaned_DS_jobs$Salary_Estimate_wo_spaces <- Uncleaned_DS_jobs$Salary_Estimate

Uncleaned_DS_jobs$Salary_Estimate_wo_spaces <- gsub(" ", "",Uncleaned_DS_jobs$Salary_Estimate)  
# Display the updated data frame 
head(Uncleaned_DS_jobs$Salary_Estimate_wo_spaces) 
```

Now we have no blank space between the words.

Let's get rid of the parts at the end; Glassdoor est. and Employer est.

```{r}
Uncleaned_DS_jobs$Salary_Estimate_wo_spaces <- 
  gsub("K\\(Glassdoorest.\\)",
       "",
       Uncleaned_DS_jobs$Salary_Estimate_wo_spaces) 
```

```{r}
Uncleaned_DS_jobs$Salary_Estimate_wo_spaces <- 
  gsub("K\\(Employerest.\\)",
       "",
       Uncleaned_DS_jobs$Salary_Estimate_wo_spaces)

head(Uncleaned_DS_jobs$Salary_Estimate_wo_spaces)
```

Let's see how we can select the numbers that are remaining in the rows: we can use \[0-9\]+ for this part:

```{r}
str_view(
  Uncleaned_DS_jobs$Salary_Estimate_wo_spaces, 
  "[0-9]+")
```

```{r}
Uncleaned_DS_jobs <- Uncleaned_DS_jobs |>    
  separate_wider_regex(     
    Salary_Estimate_wo_spaces,     
    patterns = c(
      "\\$",
      Low_Limit_For_Salary = "[0-9]+",       
      "K-\\$",       
      High_Limit_For_Salary = "[0-9]+"  
      )   
    ) 
```

By using `separate_wider_regex()` function, we defined the pattern in the data, and we got the new columns as Low_Limit_For_Salary and High_Limit_For_Salary as we wished.

```{r}
head(Uncleaned_DS_jobs[c("Salary_Estimate",
                         "Low_Limit_For_Salary", 
                         "High_Limit_For_Salary")])
```

For not confusing the numbers later, lets multiply the low limit and high limit numbers with 1000 and make Salary Estimate factor.

```{r}
Uncleaned_DS_jobs <- Uncleaned_DS_jobs %>%  
  mutate(
    Low_Limit_For_Salary = as.numeric(Low_Limit_For_Salary)*1000,
    High_Limit_For_Salary = as.numeric(High_Limit_For_Salary)*1000)
```

```{r}
Uncleaned_DS_jobs$Salary_Estimate <- as.factor(Uncleaned_DS_jobs$Salary_Estimate)

head(Uncleaned_DS_jobs[c("Salary_Estimate",
                         "Low_Limit_For_Salary", 
                         "High_Limit_For_Salary")])
```

Now we obtained two new columns as Low_Limit_For_salary and High_Limit_For_Salary for the salary estimates.

-   Job Title

For Job Title column, first let's examine it:

```{r}
glimpse(
  as.factor(
    Uncleaned_DS_jobs$Job_Title))
```

As we can see, we have 172 different levels for Job Titles. We can try to group them by searching common words.

```{r}
head(
  levels(
    as.factor(
      Uncleaned_DS_jobs$Job_Title)))
```

But before that, we can see that some columns have "Senior", "Experience" words. By using this information, we can create a new column for seniority of the job.

By using `str_view()` function, first, let's see that columns;

```{r}
str_view(
  Uncleaned_DS_jobs$Job_Title, 
  regex("^Senior|^Sr|^Experience", 
        multiline = TRUE))
```

By using `str_detect()` function, we can detect the rows including "Senior", "Sr", "Experienced" words. This function returns TRUE if they exist, and returns FALSE if they don't exist.

By using `as.integer()` , we assign 1 to exists and 0 to nonexistent.

```{r}
Uncleaned_DS_jobs$Senior_Position <- as.integer(
  str_detect(Uncleaned_DS_jobs$Job_Title, 
             regex("^(Senior|Sr|Experienced)")
             ) )
```

Now that we defined the senior roles, we can assign the same titles to same jobs.

Let's start by Data Scientist. Let's find the columns including Data Scientist word.

```{r}
str_view(Uncleaned_DS_jobs$Job_Title, 
         regex(".*Data\\s+Scientist.*", 
         multiline = TRUE))
```

Bu using `str_replace_all()` we can replace all the rows including Data Scientist word in some way, directly with "Data Scientist".

```{r}
Uncleaned_DS_jobs$Job_Title <-  
  str_replace_all(
    Uncleaned_DS_jobs$Job_Title,
    ".*Data\\s+Scientist.*",
    "Data Scientist")
```

Now let's get Data Analyst titles:

```{r}
str_view(
  Uncleaned_DS_jobs$Job_Title, 
  regex(".*Data\\s+Analyst.*", 
        multiline = TRUE,
        ignore_case = TRUE))
```

```{r}
Uncleaned_DS_jobs$Job_Title <-  
  str_replace_all(
    Uncleaned_DS_jobs$Job_Title,
    ".*Data\\s+Analyst.*",
    "Data Analyst")
```

Now let's get Data Engineer titles:

```{r}
str_view(
  Uncleaned_DS_jobs$Job_Title, 
  regex(".*Data\\s+Engineer.*", 
        multiline = TRUE,
        ignore_case = TRUE))
```

```{r}
Uncleaned_DS_jobs$Job_Title <-  
  str_replace_all(
    Uncleaned_DS_jobs$Job_Title,
    ".*Data\\s+Engineer.*",
    "Data Engineer")
```

And Machine Learning Engineers:

```{r}
str_view(
  Uncleaned_DS_jobs$Job_Title, 
  regex(".*Machine\\s+Learning.*", 
        multiline = TRUE, 
        ignore_case = TRUE))
```

```{r}
Uncleaned_DS_jobs$Job_Title <-  str_replace_all(
  Uncleaned_DS_jobs$Job_Title,
  ".*Machine\\s+Learning.*",
  "Machine Learning Engineer")
```

Let's examine Managers this time:

```{r}
str_view(
  Uncleaned_DS_jobs$Job_Title, 
  regex(".*Analytics\\s+Manager.*|.*Data\\s+Science\\sManager.*|.*Director.*|.*Vice\\sPresident.*|.*VP.*|.*Principal.*|.*Manager.*", 
        multiline = TRUE, 
        ignore_case = TRUE))
```

Let's replace them with "Data Science and Analytics Manager"

```{r}
Uncleaned_DS_jobs$Job_Title <-  
  str_replace_all(
    Uncleaned_DS_jobs$Job_Title,
    ".*Analytics\\s+Manager.*|.*Data\\s+Science\\sManager.*|.*Director.*|.*Vice\\sPresident.*|.*VP.*|.*Principal.*|.*Manager.*",
    "Data Science and Analytics Manager")
```

Now, bu using `str_view()` function, we want to see all the jobs that have "Data" in it, but not "Data Analyst", "Data Scientist,"Data Engineer" or "Data Science and Analytics Manager" because we already took care of that titles.

```{r}
str_view(
  Uncleaned_DS_jobs$Job_Title, 
  regex("^(?!.*Data\\s+Analyst.*|.*Data\\s+Scientist.*|.*Data\\s+Science\\s+and\\s+Analytics\\s+Manager.*|.*Data\\sEngineer.*).*data.*", 
        ignore_case = TRUE))
```

We will save these as "Other Data Positions"

```{r}
Uncleaned_DS_jobs$Job_Title <-  
  str_replace_all(
    Uncleaned_DS_jobs$Job_Title,
     regex("(?!Data\\s+(Analyst|Scientist|Engineer|Science\\s+and\\s+Analytics\\s+Manager)).*Data.*"),
    "Other Data Positions" )
```

Finally, we will save all the jobs that are not include "Data" word in it and not "Machine Learning Engineer" into "Others" category because there are a lot of jobs with the titles like Scientist, Researcher etc.

```{r}
Uncleaned_DS_jobs$Job_Title <-  
  str_replace_all(
    Uncleaned_DS_jobs$Job_Title, 
    regex("^(?!.*(Data|Machine\\s+Learning\\s+Engineer)).*$"), 
    "Others")
```

Finally, let's see our clean job titles:

```{r}
Uncleaned_DS_jobs$Job_Title <- as.factor(Uncleaned_DS_jobs$Job_Title)

summary(Uncleaned_DS_jobs$Job_Title)
```

Let's visualize Job Titles:

```{r}
jt_counts <- count(Uncleaned_DS_jobs, Job_Title)

# Sort the data by count in ascending order
jt_counts <- arrange(jt_counts, desc(n))

jt_plot <- ggplot(jt_counts, aes(x = reorder(Job_Title, n), y = n)) + 
   labs(title = "Distribution of Job Titles", x = "Job Titles", y = "Frequency") + 
   geom_col(colour = "darkblue", fill = "lightblue") + 
   geom_text(aes(label = n), vjust = -0.5, size = 3) +
   theme_classic() +
   theme(axis.text.x = element_text(size = 9.2, angle = 45, hjust = 1))
jt_plot
```

We can see that we have 455 "Data Scientist" postings, followed by 50 "Others", 47 "Data Engineers" , 47 "Data Analysts" and so on.

-   Job Description

When we look at the job description column,

We have so many different values but we can differentiate them into other columns like we can say that a job wants the skill SQL.

First, we need to look the job description column in a detailed way.

```{r}
head(Uncleaned_DS_jobs$Job_Description,1)
```

We see some common requirements and common job descriptions for jobs.

For this we can separate the columns like SQL and we can say that this jobs wants an SQL bu using factor 1 or 0.

Let's start with SQL:

In this we should check if SQL is mentioned in the variable Job_Description:

```{r}
sql_mentioned <- function(description) {
  # We use tolower to match the SQL in the job description
  description <- tolower(description)
  
  # Check if SQL is mentioned
  if (grepl("\\bsql\\b", description)) {
    return(1)
  } else {
    return(0)
  }
}


```

Now we need to create a column called SQL, in this column we will see if SQL is a requirement in the job description or not.

```{r}
Uncleaned_DS_jobs$sql_needed <- sapply(Uncleaned_DS_jobs$Job_Description, sql_mentioned)
```

```{r}
Uncleaned_DS_jobs$sql_needed <- as.factor(Uncleaned_DS_jobs$sql_needed)
```

Now for Python we repeat the same process.

```{r}
python_mentioned <- function(description) {
  # We use tolower to match the python in the job description
  description <- tolower(description)
  
  # Check if python is mentioned
  if (grepl("\\bpython\\b", description)) {
    return(1)
  } else {
    return(0)
  }
}
```

```{r}
Uncleaned_DS_jobs$python_needed <- sapply(Uncleaned_DS_jobs$Job_Description, python_mentioned)
```

```{r}
Uncleaned_DS_jobs$python_needed <- as.factor(Uncleaned_DS_jobs$python_needed)
```

Now for Excel:

```{r}
excel_mentioned <- function(description) {
  # We use tolower to match the excel in the job description
  description <- tolower(description)
  
  # Check if excel is mentioned
  if (grepl("\\bexcel\\b", description)) {
    return(1)
  } else {
    return(0)
  }
}
```

```{r}
Uncleaned_DS_jobs$excel_needed <- sapply(Uncleaned_DS_jobs$Job_Description, excel_mentioned)
```

```{r}
summary(Uncleaned_DS_jobs$excel_needed)
```

```{r}
Uncleaned_DS_jobs$excel_needed <- as.factor(Uncleaned_DS_jobs$excel_needed)
```

For Hadoop:

```{r}
hadoop_mentioned <- function(description) {
  # We use tolower to match the hadoop in the job description
  description <- tolower(description)
  
  # Check if hadoop is mentioned
  if (grepl("\\bhadoop\\b", description)) {
    return(1)
  } else {
    return(0)
  }
}
```

```{r}
Uncleaned_DS_jobs$hadoop_needed <- sapply(Uncleaned_DS_jobs$Job_Description, hadoop_mentioned)
```

```{r}

summary(Uncleaned_DS_jobs$hadoop_needed)

```

```{r}
Uncleaned_DS_jobs$hadoop_needed<- as.factor(Uncleaned_DS_jobs$hadoop_needed)
```

For Spark:

```{r}
spark_mentioned <- function(description) {
  # We use tolower to match the spark in the job description
  description <- tolower(description)
  
  # Check if spark is mentioned
  if (grepl("\\bspark\\b", description)) {
    return(1)
  } else {
    return(0)
  }
}
```

```{r}
Uncleaned_DS_jobs$spark_needed <- sapply(Uncleaned_DS_jobs$Job_Description, spark_mentioned)
```

```{r}
summary(Uncleaned_DS_jobs$spark_needed)
```

```{r}
Uncleaned_DS_jobs$spark_needed <- as.factor(Uncleaned_DS_jobs$spark_needed)
```

For AWS:

```{r}
aws_mentioned <- function(description) {
  # We use tolower to match the AWS in the job description
  description <- tolower(description)
  
  # Check if AWS is mentioned
  if (grepl("\\baws\\b", description)) {
    return(1)
  } else {
    return(0)
  }
}

```

```{r}
Uncleaned_DS_jobs$aws_needed <- sapply(Uncleaned_DS_jobs$Job_Description, aws_mentioned)
```

```{r}
summary(Uncleaned_DS_jobs$aws_needed)
```

```{r}
Uncleaned_DS_jobs$aws_needed <- as.factor(Uncleaned_DS_jobs$aws_needed)
```

For Tableau:

```{r}
tableau_mentioned <- function(description) {
  # We use tolower to match the Tableau in the job description
  description <- tolower(description)
  
  # Check if Tableau is mentioned
  if (grepl("\\btableau\\b", description)) {
    return(1)
  } else {
    return(0)
  }
}
```

```{r}
Uncleaned_DS_jobs$tableau_needed <- sapply(Uncleaned_DS_jobs$Job_Description, tableau_mentioned)
```

```{r}
summary(Uncleaned_DS_jobs$tableau_needed)
```

```{r}
Uncleaned_DS_jobs$tableau_needed <- as.factor(Uncleaned_DS_jobs$tableau_needed)
```

For Big Data:

```{r}
bigdata_mentioned <- function(description) {
  # We use tolower to match the Big data in the job description
  description <- tolower(description)
  
  # Check if Big data is mentioned
  if (grepl("\\bbig-data\\b", description)) {
    return(1)
  } else {
    return(0)
  }
}
```

```{r}
Uncleaned_DS_jobs$bigdata_needed <- sapply(Uncleaned_DS_jobs$Job_Description, bigdata_mentioned)
```

```{r}
summary(Uncleaned_DS_jobs$bigdata_needed)
```

```{r}

Uncleaned_DS_jobs$bigdata_needed<- as.factor(Uncleaned_DS_jobs$bigdata_needed)
```

For Numpy:

```{r}
numpy_mentioned <- function(description) {
  # We use tolower to match the Numpy in the job description
  description <- tolower(description)
  
  # Check if Numpy is mentioned
  if (grepl("\\bnumpy\\b", description)) {
    return(1)
  } else {
    return(0)
  }
}
```

```{r}
Uncleaned_DS_jobs$numpy_needed <- sapply(Uncleaned_DS_jobs$Job_Description, numpy_mentioned)
```

```{r}
summary(Uncleaned_DS_jobs$numpy_needed)
```

```{r}
Uncleaned_DS_jobs$numpy_needed <- as.factor(Uncleaned_DS_jobs$numpy_needed)
```

For Machine Learning:

```{r}
ML_mentioned <- function(description) {
  # We use tolower to match the ML in the job description
  description <- tolower(description)
  
  # Check if ML is mentioned
  if (grepl("\\bmachine learning\\b", description)) {
    return(1)
  } else {
    return(0)
  }
}
```

```{r}
Uncleaned_DS_jobs$ML_needed <- sapply(Uncleaned_DS_jobs$Job_Description, ML_mentioned)
```

```{r}
summary(Uncleaned_DS_jobs$ML_needed)
```

```{r}
Uncleaned_DS_jobs$ML_needed<- as.factor(Uncleaned_DS_jobs$ML_needed)
```

For Deep Learning:

```{r}
DL_mentioned <- function(description) {
  # We use tolower to match the DL in the job description
  description <- tolower(description)
  
  # Check if DL is mentioned
  if (grepl("\\bdeep learning\\b", description)) {
    return(1)
  } else {
    return(0)
  }
}
```

```{r}
Uncleaned_DS_jobs$DL_needed <- sapply(Uncleaned_DS_jobs$Job_Description, DL_mentioned)
```

```{r}
summary(Uncleaned_DS_jobs$DL_needed)
```

```{r}
Uncleaned_DS_jobs$DL_needed <- as.factor(Uncleaned_DS_jobs$DL_needed)
```

For Statistics:

```{r}
stat_mentioned <- function(description) {
  # We use tolower to match the statistics in the job description
  description <- tolower(description)
  
  # Check if statistics is mentioned
  if (grepl("\\bstatistics\\b", description)) {
    return(1)
  } else {
    return(0)
  }
}
```

```{r}
Uncleaned_DS_jobs$stat_needed <- sapply(Uncleaned_DS_jobs$Job_Description, stat_mentioned)
```

```{r}
summary(Uncleaned_DS_jobs$stat_needed)
```

```{r}
Uncleaned_DS_jobs$stat_needed <- as.factor(Uncleaned_DS_jobs$stat_needed)
```

Now Let's check the new columns in our dataset:

```{r}
summary(Uncleaned_DS_jobs)
```

## Visualizations

First, let's see our variables:

```{r}
glimpse(Uncleaned_DS_jobs)
```

-   **Salary Distribution by Job Titles:** Let's display the distribution of salaries for different job titles; both for low limit estimate for salary and high limit estimate for salary:

-   Low Limit Estimate for Salary

```{r}
salary_distribution <- ggplot(Uncleaned_DS_jobs, aes(x = Job_Title, y = Low_Limit_For_Salary)) +
  geom_boxplot() +
  labs(title = "Low Limit Salary Distribution by Job Title",
       x = "Job Title",
       y = "Low Limit for Salary") +
  theme_classic() +
  theme(axis.text.x = element_text(size = 9.2, angle = 45, hjust = 1))

salary_distribution

```

From the plot, we can see that the medians are not really different from each other for the several Job Titles. We can see that, interestingly, "Other" type of jobs seem to have a higher median in their salaries for the low limit compared to the jobs including "Data" in their titles.

```{r}
# Calculate median low salary limits for each job title
salary_median <- Uncleaned_DS_jobs %>%
  group_by(Job_Title) %>%
  summarise(median_salary = median(Low_Limit_For_Salary))

# Sort the data by median low salary in descending order
salary_median <- salary_median %>%
  arrange(desc(median_salary))


salary_distribution <- ggplot(salary_median, 
                              aes(x = reorder(Job_Title, 
                                              median_salary), 
                                  y = median_salary)) +
  geom_bar(stat = "identity", fill = "lightblue", col = "darkblue") +
  geom_text(aes(label = scales::comma(median_salary)), vjust = -0.5, size = 3) +
  labs(title = "Median Low Limit for Salary by Job Title",
       x = "Job Title",
       y = "Median Low Limit for Salary") +
  theme_classic() +
  theme(axis.text.x = element_text(size = 9.2, angle = 45, hjust = 1))

salary_distribution


```

Just to be sure, we would like to check the medians in a bar graph anyway. From the graph, as we suggested earlier, we can see that "Other" job titles indeed have a higher median in their pays with 100K.

Let's examine the High Limit Estimate for Salary:

-   High Limit Estimate for Salary

```{r}
salary_distribution_high <- ggplot(Uncleaned_DS_jobs, aes(x = Job_Title, y = High_Limit_For_Salary)) +
  geom_boxplot() +
  labs(title = "High Limit Salary Distribution by Job Title",
       x = "Job Title",
       y = "High Limit for Salary") +
  theme_classic() +
  theme(axis.text.x = element_text(size = 9.2, angle = 45, hjust = 1))

salary_distribution_high
```

From the box plot of the pays, as may be expected, "Data Science and Analytics Managers" have the highest median in pay compared to the other job titles. But we can see that we also have outliers in both managers, as well as other jobs too. Let's see them just out of curiosity:

```{r}
# Arrange the data by 'Job_Title' and 'Salary' in descending order
sorted_data <- Uncleaned_DS_jobs %>% dplyr::arrange(Job_Title, desc(High_Limit_For_Salary))

# Filter to keep the rows with the highest salary for each job title
highest_salary_rows <- sorted_data %>% group_by(Job_Title) %>% slice(1)

# Print the filtered rows
highest_salary_rows |> select(Job_Title, Salary_Estimate,Company_Name)

```

Here we can see the highest salary range for each of the job titles.

```{r}
# Calculate median low salary limits for each job title
salary_median <- Uncleaned_DS_jobs %>%
  group_by(Job_Title) %>%
  summarise(median_salary = median(High_Limit_For_Salary))

# Sort the data by median low salary in descending order
salary_median <- salary_median %>%
  arrange(desc(median_salary))


salary_distribution <- ggplot(salary_median, 
                              aes(x = reorder(Job_Title, 
                                              median_salary), 
                                  y = median_salary)) +
  geom_bar(stat = "identity", fill = "lightblue", col = "darkblue") +
  geom_text(aes(label = scales::comma(median_salary)), vjust = -0.5, size = 3) +
  labs(title = "Median High Limit for Salary by Job Title",
       x = "Job Title",
       y = "Median High Limit for Salary") +
  theme_classic() +
  theme(axis.text.x = element_text(size = 9.2, angle = 45, hjust = 1))

salary_distribution

```

As can be seen from the medians graph that there is not a significant difference between the medians of the different job titles but Data Science and Analytics Managers have the highest median pay with 147K.

-   **Company Size vs. Salaries:** To explore how salary ranges vary across different company sizes:

    ```{r}
    # Create a new variable with sorted factor levels
    data_sorted <- transform(Uncleaned_DS_jobs, Size_Sorted = factor(Size, levels = sort(unique(Size))))

    # Create boxplot for Low_Limit_For_Salary with adjusted y-axis limits
    plot_low <- ggplot(data_sorted, aes(x = Size_Sorted, y = Low_Limit_For_Salary, fill = factor(Size_Sorted))) +
      geom_boxplot(alpha = 0.8) +
      labs(title = "Low Limit Salary Ranges across Company Sizes",
           x = "Company Size",
           y = "Low Limit Salary") +
      scale_fill_discrete(name = "Company Size") +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
      coord_cartesian(ylim = c(0, 340000))  # Set y-axis limits

    # Create boxplot for High_Limit_For_Salary with the same y-axis limits
    plot_high <- ggplot(data_sorted, aes(x = Size_Sorted, y = High_Limit_For_Salary, fill = factor(Size_Sorted))) +
      geom_boxplot(alpha = 0.8) +
      labs(title = "High Limit Salary Ranges across Company Sizes",
           x = "Company Size",
           y = "High Limit Salary") +
      scale_fill_discrete(name = "Company Size") +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
      coord_cartesian(ylim = c(0, 340000))  # Set y-axis limits

    plot_low
    plot_high
    ```

From the boxplot of the High and Low Limit Ranges for the salaries versus company sizes, we can see that 5001 to 10000 employes have the highest median in the high limit salary ranges.

-   **Job Titles vs. Senior Positions:** Visualize the proportion of senior positions against different job titles using a bar chart:

Firstly, we can see the distribution of Senior Position among the job titles:

```{r}
Uncleaned_DS_jobs %>% 
  select(Senior_Position, Job_Title)  %>% 
  group_by(Job_Title, Senior_Position)  %>% 
  count()
```

Then we will calculate the percentage of the senior positions for every title:

```{r}
senior_proportion <- Uncleaned_DS_jobs %>%
  group_by(Job_Title) %>%
  summarise(Percentage_Senior = mean(Senior_Position) * 100) %>%
  arrange(desc(Percentage_Senior))
senior_proportion
```

Let's create bar graph:

```{r}
senior_plot <- ggplot(senior_proportion, aes(x = reorder(Job_Title, Percentage_Senior), y = Percentage_Senior)) +
  geom_bar(stat = "identity", fill = "skyblue", alpha = 0.8) +
  geom_text(aes(label = paste0(round(Percentage_Senior), "%")), vjust = -0.5, size = 3.5, color = "black") +
  labs(title = "Proportion of Senior Positions by Job Title",
       x = "Job Title",
       y = "Percentage of Senior Positions") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

senior_plot
```

We can see that the title that is searched for seniority is Data Analyst, however, only 21% of the Data Analyst positions are senior.

-   **Industry Analysis:** Use a treemap to display the distribution of job positions across different industries.

```{r}
library(treemap)

job_count_by_industry <- Uncleaned_DS_jobs %>%
  group_by(Industry) %>%
  summarise(Job_Count = n()) %>%
  arrange(desc(Job_Count))

# Create a treemap for job positions across different industries with custom theme
treemap_plot <- treemap(job_count_by_industry, index = "Industry", vSize = "Job_Count",
                        title = "Distribution of Job Positions across Industries",
                        palette = "Blues")
```

We can see that the industries that are hiring the most are: Biotech & Pharmaceuticals, IT Services, Computer Hardware & Software, Aerospace & Defense and so on.

-   Revenue vs. Ratings: Create a grouped boxplot to show the distribution of ratings for different revenue categories.

```{r}
boxplot <- ggplot(Uncleaned_DS_jobs, 
                  aes(x = Revenue, 
                      y = Rating, 
                      fill = Revenue)) +
  geom_boxplot(alpha = 0.8) +
  labs(title = "Rating Distribution across Revenue Categories",
       x = "Revenue Categories",
       y = "Rating") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

boxplot
```

From the plot we can see that highest median rating for the companies according to their revenues was for the companies with \$25 to \$50 million (USD).

-   Overview of Skills Needed in the Job Postings:

```{r}
skills_df <- Uncleaned_DS_jobs[, c("sql_needed", "python_needed", "numpy_needed", "stat_needed","excel_needed","hadoop_needed","spark_needed",
                    "aws_needed","tableau_needed","bigdata_needed","ML_needed","DL_needed")]

skills_long <- tidyr::gather(skills_df, key = "Skill", value = "Needed")


my_colors <- c("lightblue", "darkblue")

skills_long$Needed <- factor(skills_long$Needed, levels = c("0", "1"))

skills_plot <- ggplot(skills_long, aes(x = Skill, fill = Needed)) +
  geom_bar(position = "dodge", width = 0.7) +
  scale_fill_manual(values = my_colors) + 
  labs(title = "Overview of Skills Needed",
       x = "Skill",
       y = "Count") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

skills_plot
```

As can be seen from the graph that python and ML are most mentioned skills that are required in the job postings.

## Conclusion

In conclusion, for this data handling and visaulization project, we use the data from Kaggle named "Data Science Job Posting on Glassdoor". This data set is originally obtained from Glassdoor website by using web scrapping. We started our analysis by data cleaning. We tidied the columns by separating the columns that requiring more than one information. We grouped job titles by their similarities and we obtained new variables from them. At start, we had a total of 672 rows with 15 columns but at the end, we managed to have 31 columns from all the information we have found within the data set. As for findings, when we investigate the lower limit of salaries for the job postings, we saw that "Other" job titles, meaning jobs that does not have "data" in their titles at all, jobs like scientist, researcher etc. have a higher median in their pays with 100K. However, for the high limit for the salaries are investigated, as may be expected, "Data Science and Analytics Managers" have found to be the highest median in pay compared to the other job titles. When we examine High and Low Limit Ranges for the salaries versus company sizes, we can see that 5001 to 10000 employes have the highest median in the high limit salary ranges. Among the jobs postings 21% of the Data Analyst positions are found as senior position. We saw that the industries that are hiring the most are: Biotech & Pharmaceuticals, IT Services, Computer Hardware & Software, Aerospace & Defense and so on. Futhermore, highest median rating for the companies according to their revenues was for the companies with \$25 to \$50 million (USD). Finally, Python and ML are most mentioned skills that are required in the job postings.

## References

About us \| glassdoor. (n.d.). <https://www.glassdoor.com/about/>

Barr, D., & DeBruine, L. (n.d.). Data Cleaning. <https://rgup.gitlab.io/research_cycle/03_tidyr.html>

Cotton, R. (2023, February 16). *Quarto cheat sheet (previously known as RMarkdown)*. DataCamp. <https://www.datacamp.com/cheat-sheet/quarto-cheat-sheet-previously-known-as-r-markdown>

*GREP: Pattern matching and replacement*. RDocumentation. (n.d.). <https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/grep>

Rahman, R. (n.d.). \[Dataset\] Data Science Job Posting on Glassdoor. Retrieved December 20, 2023,. <https://www.kaggle.com/datasets/rashikrahmanpritom/data-science-job-posting-on-glassdoor/data>

Wickham, H., Çetinkaya-Rundel, M., & Grolemund, G. (2023). *R for Data Science: Import, Tidy, transform, visualize, and model data*. O'Reilly Media, Inc.
